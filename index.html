<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- 描述信息，显示在搜索引擎结果中，可以根据实际内容更改 -->
  <meta name="description"
        content="Machine Learning Modeling for Multi-order Human Visual Motion Perception">
  <!-- 关键词，用于搜索引擎优化，可根据实际内容调整 -->
  <meta name="keywords" content="Visual motion perception, Optical flow, Second-order motion, Graph neural network, Motion segmentation">
  <!-- 视口设置，让页面在移动设备上更好显示 -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- 网页标题，将显示在浏览器标签页上，可自定义 -->
  <title>MotionModel</title>

  <!-- Google Analytics 跟踪代码，用于网站流量统计 -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- 加载Google字体，可根据需要修改 -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- 加载CSS样式文件 -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- 页面图标 -->
  <link rel="icon" href="./static/images/eye.svg">

  <!-- 加载jQuery和其他JS库 -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- 导航栏 -->
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <!-- 菜单按钮 -->
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- 首页图标，可修改链接 -->
      <a class="navbar-item" href="xxxx">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>

      <!-- 更多研究的下拉菜单 -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More Research  </a>
        <div class="navbar-dropdown">
         
          <a class="navbar-item" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4c9477b9e2c7ec0ad3f4f15077aaf85a-Paper-Conference.pdf">Motion Model</a>
          <a class="navbar-item" href="https://www.cell.com/iscience/pdf/S2589-0042(23)02384-2.pdf">Human perceived flow</a>
        </div>
      </div>
    </div>
  </div>
</nav>
  
<!-- 标题和作者信息部分 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- 主要标题，页面显示的标题，可自定义 -->
          <h1 class="title is-1 publication-title colorful-title">Machine Learning Modeling for Multi-order Human Visual Motion Perception</h1>
          <div class="is-size-5 publication-authors">
            <!-- 作者名和链接，可修改成实际作者信息 -->
            <span class="author-block">
              Zitang Sun </a><sup>1</sup>,
              Yen-Ju Chen </a><sup>1</sup>,
              Yung-hao Yang </a><sup>1</sup>,
              Yuan Li </a><sup>1</sup>,
              <a href="https://scholar.google.com/citations?user=bxhQU8EAAAAJ&hl=en"> Shin'ya Nishida* </a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- 作者所属机构信息 -->
            <span class="author-block"><sup>1</sup>Cognitive Informatics Lab, Kyoto University, Japan,</span>
          
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- 论文链接，点击时弹出提示 -->
              <span class="link-block">
                <a href="javascript:void(0);" onclick="showAlert()" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="javascript:void(0);" onclick="showAlert()" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- 视频链接，点击时弹出提示 -->
              <span class="link-block">
                <a href="javascript:void(0);" onclick="showAlert()" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-youtube"></i></span>
                  <span>Video</span>
                </a>
              </span>
              <!-- 代码链接，点击时弹出提示 -->
              <span class="link-block">
                <a href="javascript:void(0);" onclick="showAlert()" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <!-- 数据集链接，点击时弹出提示 -->
              <span class="link-block">
                <a href="javascript:void(0);" onclick="showAlert()" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>Data</span>
                </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  /* 标题居中对齐 */
  .colorful-title {
    text-align: center;
    font-size: 2.5em; /* 根据需要调整字体大小 */
  }

  /* 为不同颜色定义类 */
  .word-color-1 { color: #ff5733; }
  .word-color-2 { color: #33c1ff; }
  .word-color-3 { color: #9b33ff; }
  .word-color-4 { color: #33ff57; }
  .word-color-5 { color: #ffd433; }
  .word-color-6 { color: #ff33a6; }
</style>

<script>
  // JavaScript代码：将标题的每个单词分配随机颜色
  document.addEventListener("DOMContentLoaded", function() {
    const title = document.querySelector(".colorful-title");
    const words = title.innerText.split(" ");
    title.innerHTML = words.map((word, index) => {
      const colorClass = `word-color-${(index % 6) + 1}`; // 分配颜色类
      return `<span class="${colorClass}">${word}</span>`;
    }).join(" ");
  });
</script>


<!-- JavaScript，用于显示提示信息 -->
<script>
  function showAlert() {
    alert("Not available currently. All data/code/benchmark will be open once the paper gets accepted.");
  }
</script>

<style>
  .scaled-video {
    width: 70vw; /* 占据视口宽度的 70% */
    height: 70vh; /* 占据视口高度的 70% */
    object-fit: contain; /* 保持视频内容完整，可能会有空白边 */
    display: block; /* 使视频为块级元素，以便居中 */
    margin: 0 auto; /* 水平居中对齐 */
  }
 .overlay-text {
    text-align: center;
    color: white;
    background: rgba(0, 0, 0, 0.5);
    padding: 10px;
    border-radius: 5px;
    max-width: 70%;
    margin-top: -20px; /* 仅调整顶部间距 */
    margin-left: auto;
    margin-right: auto; /* 水平居中 */
}
</style>


<style>
  /* 视频样式 */
  .video-element {
    width: 90vw; /* 占据视口宽度的90% */
    height: auto; /* 保持视频宽高比 */
    margin: 0 auto; /* 水平居中 */
    display: block;
  }

  /* 调整视频说明的样式 */
  figcaption {
    font-size: 1.3em;
    margin-top: 10px;
    color: #555;
    text-align: center;
  }
</style>

<style>
  /* 视频样式 */
  .video-rec {
    width: 60vw; /* 占据视口宽度的90% */
    height: auto; /* 保持视频宽高比 */
    margin: 0 auto; /* 水平居中 */
    display: block;
  }

  /* 调整视频说明的样式 */
  figcaption {
    font-size: 1.3em;
    margin-top: 10px;
    color: #555;
    text-align: center;
  }
</style>


<style>
  /* 视频样式 */
  .video-left {
    width: 90vw; /* 占据视口宽度的90% */
    height: auto; /* 保持视频宽高比 */
    margin: 0 auto; /* 水平居中 */
    display: block;
  }

  /* 调整视频说明的样式 */
  figcaption {
    font-size: 1.5em; /* 调整字体大小 */
    margin-bottom: 10px; /* 与视频间距 */
    color: #555; /* 文本颜色 */
    text-align: left; /* 左对齐文字 */
  }

  /* 视频容器样式 */
  .video-container {
    display: flex;
    flex-direction: column; /* 垂直排列 */
    align-items: center; /* 水平居中 */
    text-align: center; /* 容器内文字居中 */
  }

  /* 确保文字宽度与视频一致 */
  .video-container figcaption {
    width: 90vw; /* 与视频宽度一致 */
    margin: 0 auto 10px auto; /* 保持水平居中 */
  }
</style>


<style>
  /* 图像样式 */
  .image-element {
    width: 90vw; /* 占据视口宽度的90% */
    height: auto; /* 保持图像宽高比 */
    margin: 0 auto; /* 水平居中 */
    display: block;
  }

  /* 调整图像说明的样式 */
  .caption-text {
    max-width: 90vw; /* 限制文字宽度与图像一致 */
    font-size: 1.3em; /* 调整字体大小 */
    margin-bottom: 10px; /* 与图像的间距 */
    color: #555; /* 文字颜色 */
    text-align: left; /* 左对齐文字 */
    word-wrap: break-word; /* 防止文字超出范围 */
    overflow-wrap: break-word; /* 增强兼容性 */
  }

  /* 图像容器样式 */
  .image-container {
    display: flex;
    flex-direction: column; /* 垂直排列 */
    align-items: center; /* 水平居中 */
    text-align: center; /* 容器内文字居中 */
  }
</style>

<style>
  /* 视频样式 */
  .video-element {
    width: 90vw; /* 占据视口宽度的90% */
    height: auto; /* 保持视频宽高比 */
    margin: 0 auto; /* 水平居中 */
    display: block;
  }

  /* 调整视频说明的样式 */
  figcaption {
    font-size: 1.3em; /* 调整字体大小 */
    margin-bottom: 10px; /* 与视频间距 */
    color: #555; /* 文本颜色 */
    text-align: left; /* 左对齐文字 */
  }

  /* 视频容器样式 */
  .video-container {
    display: flex;
    flex-direction: column; /* 垂直排列 */
    align-items: center; /* 水平居中 */
    text-align: center; /* 容器内文字居中 */
  }

  /* 确保文字宽度与视频一致 */
  .video-container figcaption {
    width: 90vw; /* 与视频宽度一致 */
    margin: 0 auto 10px auto; /* 保持水平居中 */
  }
</style>


<hr class="gradient-divider">

<style>
  .gradient-divider {
    border: none; /* 移除默认边框 */
    height: 6px; /* 设置分隔线粗细 */
    background: linear-gradient(to right, red, orange, yellow, green, blue, indigo, violet); /* 彩虹渐变颜色 */
    margin: 20px 0; /* 上下间距 */
    border-radius: 3px; /* 圆角效果 */
  }
</style>
<hr class="gradient-divider">

<h2 class="title is-3">     Introduction</h2>
  
  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption> Visual motion perception and estimation is an essential function for living things or AI agent to interact with this dynamic world , 
        as it supports essential tasks such as navigation, tracking, prediction, and pursuit.

      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/1_intro.mp4" type="video/mp4">
        Your browser does not support video tags.
      </video>
      
    </figure>
  </div>
</section>

<hr class="section-divider">
  
<section class="section">
  <div class="container">
    <!-- 使用 Columns 布局 -->
    <div class="columns is-vcentered">
      <!-- 左侧视频部分 -->
      <div class="column is-half">
        <figure>
       <figcaption> Recent advances in deep learning have propelled machine vision capabilities to a level comparable or superior to human performance in
                  many vision tasks. Visual motion estimation is not an exception; state-of-the-art (SOTA)
                  computer vision (CV) models based on deep
                  learning has demonstrated higher accuracy than
                  that of humans in computing ground-truth optical flow from images. However, these models
                  are not sufficiently human-aligned. Strutruclly, human motion perception Involves a complicated spatiotemporal dynamic process, but the current CV models mainly focus on the correspondence tracking between two frames.
                  the different design purpose also let cv models somehow ignore neurophysiological plausibility.
         
                  Perceptually, CV models being unable
                  to predict human motion perception in many
                  aspects. For example, CV models are often unstable under various experimental conditions,
                  fail to reproduce human visual illusions, and do
                  not fully capture the biases inherent in human
                  perception. See left two examples of the motion integration with depth cues and mario reverse-phi illusion.

          
          <video class="video-element" autoplay loop muted playsinline>
            <source src="./data_for_website/2_gap_1.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
      </div>
      <!-- 右侧文字部分 -->
      <div class="column is-half content">
        <p> .
        </p>
      </div>
    </div>
  </div>
</section>



  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        Among the varios Gap between CV models and humans, one of the significant gap is about second-order motion processing and perception. As shown below,
        We humans have a similar perception of the moving speed and direction of these two stimuli. However, they have different characteristics in frequency space.
A simple clarification is that if you carry out the Fourier Transformation of their spatiotemporal map, you will find the first-order motion has  asymmetric energy in the Fourier Space 
But the second-order motion doesn’t exhibit any visible or meaningful energy in Fourier Space, which means that it’s not detectable by the classical Fourier analysis.

        From mechiasm, human use differnt mechiems or channls to processing the second-order motion while the CV models didnt consider that. 
        From response, its also obvous that the CV model failed to capture this type of motion and generate noise-like response. The reason is,most CV models primarily focus on tracking explicit
pixel correspondences between frames, relying
mainly on brightness consistency. Most
second-order motions, such as drift-balanced
motion, lack such deterministic correspondences across frames,
leading current machine vision models to become
unstable and generate noise, even they trained sufficiently on this types of motions.

        

      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/2_gap_2.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      
    </figure>
  </div>
</section>



<hr class="gradient-divider">

  <!-- 摘要部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- 论文摘要内容，可以自行替换 -->
          <p>
           The objective of this study is to develop machines that naturally learn to perceive visual motion as humans do. 
            While recent advances in computer vision (CV) have enabled DNN-based models to accurately estimate optical flow in naturalistic images, 
            a significant disparity remains between CV models and the biological visual system in both architecture and behavior. 
            This disparity includes humans' ability to perceive the motion of higher-order image features (second-order motion), 
            which many CV models fail to capture due to their reliance on the intensity conservation law. 
            Our model architecture mimics the cortical V1-MT motion processing pathway, utilizing a trainable motion energy sensor bank and a recurrent self-attention network. 
            Supervised learning on diverse naturalistic movies allows the model to replicate psychophysical and physiological findings about first-order (luminance-based) motion perception. 
            For second-order motion, inspired by neuroscientific findings, the model includes an additional sensing pathway with nonlinear preprocessing before motion energy sensing, 
            implemented using a simple multi-layer 3D CNN block. 
            To explore how the brain naturally acquires second-order motion perception in natural environments---where pure second-order signals are rare---we hypothesized 
            that second-order mechanisms are critical for estimating robust object motion amidst optical fluctuations, such as highlights on glossy surfaces. 
            We trained our dual-pathway model on novel motion datasets with varying material properties of moving objects and found that 
            training to estimate motion for non-Lambertian materials naturally endowed the model with the ability to perceive second-order motion, 
            akin to humans. The resulting model effectively aligns with biological systems while generalizing to both first- and second-order motion phenomena in natural scenes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<hr class="gradient-divider">
<h2 class="title is-3">     Two-Stages-Dual-Channel Model Design</h2>
<section class="section">
  <div class="container">
    <!-- 图像及文字说明 -->
    <figure class="image-container">
      <figcaption>
        Our prototype model features two-stage motion processing that combines classical ME sensors in Stage I with modern DNNs in Stage II. 
        Stage I captures local ME, simulating the function of the V1 area. In contrast, Stage II focuses on global motion integration and segregation and thus simulates the primary function of the MT area.
         The red route in Fig below is that of classical first-order motion. Specifically, we built 256 trainable ME units, each with a quadrature 2D Gabor spatial filter and a quadrature temporal filter. 
        These captured the spatiotemporal motion energies of input videos within a multi-scale wavelet space. The motion energy channel is limited to capturing first-order motion. To enable the extraction
        of higher-order motion information, we introduce an alternative channel, depicted by the gray
        route in Fig below, which uses multi-layer 3D
        CNNs ahead of motion energy computation to
        extract nonlinear spatiotemporal features. Such design is Inspired by earlier studies on human vision suggesting separate processing mechanisms for first- and second-order motion.
      </figcaption>
      <img src="./data_for_website/3_model_1.png"  class="image-element">
    </figure>
  </div>
</section>

  <hr class="section-divider">

  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        The second stage, which mimics the middle temporal cortex (MT), addresses motion integration and segregation. 
        We introduce a graph network to construct a motion graph of a dynamic scene, enabling flexible connections between local motion elements and recurrent global motion integration. 
        Efficient motion integration aside, the motion graph implicitly encodes object interconnections in a graph topology. 
        Training-free graph cuts\cite{shi2000normalized} can be seamlessly applied for object-level segmentation. See below for The demo of the inferred motion flow field and instance generate from the the motion grah.
      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/3_model_2.mp4" type="video/mp4">
        Your browser does not support video tags.
      </video>
      
    </figure>
  </div>
</section>

<hr class="gradient-divider">
<h2 class="title is-3">    Dataset</h2>

<section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        For second-order motion part, Given the computational power of neural networks, 
        the modified model was expected to detect second-order motion after training on an adequate number of artificial, 
        second-order motion stimuli. However, such training is unrealistic in natural environments, where pure second-order motions are rarely observed. 
        The critical scientific question is how and why the biological visual system naturally acquires the ability to perceive second-order motion. 
         We hypothesized that second-order motion perception aided the estimation of the motion of objects exhibiting different material properties.
        Before diving into our argument,  offer some demos of the different materials and their optical properties.
        As you can see video below, different reflection and transmission properties will cause different optical trajectories, such as diffuse, transmission, and specular.
        If you look in detail at the object’s surface, you will find that material with diffuse reflection shows a reliable optical flow.
This is because they follow the constant brightness assumption, and local brightness shifts closely correspond to the actual object’s motion. First-order motion detectors can effectively capture such shifts.

However, when materials with non-diffuse reflections, such as transmission or specular, would generate unpredictable optical turbulence or noise on the surface. 
In this case, the local luminance shifts are not yet trustable to infer the object motion, which will challenge the first-order motion system. However, we humans still can correctly perceive the movement of these non-diffuse objects. Our assumption is that the vision system may employ some higher-order motion processing, like second-order motion perception, to help estimate the object motion despite optical turbulence.


      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/4_dataset_1.mp4" type="video/mp4">
        Your browser does not support video tags.
      </video>
      
    </figure>
  </div>
</section>

  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        To test the hypothesis, we rendered a new motion dataset with different materials, 
        We generate a large motion dataset that separates the material’s properties into two groups: diffuse material and non-diffuse material. 

Our critical strategy is to simulate a biological agent/ model that learns or interacts in different environments; we can verify whether the properties of materials have an influence on second-order motion perception.
        We used supervised training to let the model learn to estimate the object motion in diffuse and non-diffuse digital videos, 
We provide the label of the object's motion and ignore its local optical turbulence. We train the model until it converges.
        Visually, it shows that our agent trained on a non-diffuse dataset can perceive second-order motion like human beings. 
        That may indicate that the training to recognize the motion of non-diffuse objects can trigger the model naturally recognizing the second-order motion.

      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/4_dataset_2.mp4" type="video/mp4">
        Your browser does not support video tags.
      </video>
      
    </figure>
  </div>
</section>


  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        In the next step, we would like to quantify the ability of the second-order motion perception.
To achieve this goal, we will compare the motion estimation performance between humans and models on the same datasets.
But, currently, there is no second-order motion dataset with diverse samples and abunt scenes or situation for second-order motion. 
Therefore, we created a standard dataset that incorporates second-order motion into naturalistic backgrounds, and we designed several types of realistic modulations.  Such as water wave dynamics and swirl modulation, as we show below.
        
      </figcaption>
      <video class="video-rec" autoplay loop muted playsinline>
        <source src="./data_for_website/4_dataset_3.mp4" type="video/mp4">
        Your browser does not support video tags.
      </video>
      
    </figure>
  </div>
</section>



  
  
<hr class="section-divider">
<!-- 轮播视频展示 -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- 描述文字 -->
      <p class="has-text-centered is-size-5 mb-4">Swipe to browse more examples of second-order motion</p>

      <div id="results-carousel" class="carousel results-carousel">
        <!-- 每个视频项目 -->
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/1_KITTI_Session5_Mov1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/2_VirtualKITTI2_Session4_Mov2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/3_MPI_Sintel_NV_Session4_Mov1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/4_VIPER_Session7_Mov2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/5_Spring_Session12_Mov2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/6_Monkaa_Session4_Mov1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/7_MHOF_Session7_Mov2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/8_Driving_Session12_Mov2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/9_Flyingthings3D_Session9_Mov1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/10_TartanAir_Session2_Mov2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  /* 样式用于让描述文字居中并适应页面风格 */
  .has-text-centered {
    text-align: center;
  }
  .is-size-5 {
    font-size: 1.3em;
  }
  .mb-4 {
    margin-bottom: 1.5rem;
  }
</style>

<hr class="gradient-divider">
<h2 class="title is-3">    Human Experiment</h2>

  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        Based on this second-order dataset, we employ our psychophysical method to collect human-perceived second-order motion vectors.
Let me show some demo trials here: Human subjects need seat in front of the monitor to use the mouse to control the moving noise to match the second-order motion they perceived.
        The stimlie will reprete multiple times to let sufficient time for inference
Three  Our psychophysical experiment revealed a strong correlation between 
        the physical GT and the human response in terms of detecting second-order motion $(r_{mean}=0.983, sd=0.005)$ (Fig. \ref{fsecdata}-C). 
        In contrast, a representative CV model, RAFT, was associated with a much lower correlation $(r=0.102)$,
        this can not be improved enven trained on our non-diffuse dataset since the sturctue limitaion of CV models.  
        Instead, Training of our dual-channel model using non-diffuse materials substantially improved recognition of second-order motion.
        The average correlation was $0.902$, very close to that of human perceptional data 
        
      </figcaption>
      <video class="video-element" autoplay loop muted playsinline>
        <source src="./data_for_website/4_dataset_4.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      
    </figure>
  </div>
</section>



<hr class="gradient-divider">
<h2 class="title is-3">    Scientific Hypothesis</h2>

  <hr class="section-divider">
  <section class="section">
  <div class="container has-text-centered">
    <figure>
      <figcaption>
        One of the primary goals of the motion perception system is to infer a reliable object’s motion.
but the Optical turbulence in the natural environment will severely make the estimation from the first order very unreliable.
Since the emergence of second-order motion perception is highly related to object motion with optical turbulence. 
From this observation, we hypothesize the natural environment may pushes the vision system to develop a new mechanism to counter strong optical turbulence and noise.
This countering process might, at least partially, lead to the evolution of second-order motion perception.

      </figcaption>
      <video class="video-rec" autoplay loop muted playsinline>
        <source src="./data_for_website/5_conclu.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      
    </figure>
  </div>
</section>


<hr class="gradient-divider">
<h2 class="title is-3">    More Demos </h2>
  
<!-- Teaser 主视频展示 -->
<section class="hero teaser">
  <video id="teaser" autoplay muted loop playsinline class="scaled-video">
    <source src="./data_for_website/6_firsec.mp4" type="video/mp4">
     Your browser does not support the video tag.
  </video>

  
  <h2 class="subtitle has-text-centered overlay-text">
   we presented some examples of the natural environment to highlight our argument, such as the water flowing in the river or the moving box with the vibration water.
 We humans can perceive the local optical noise generated by fluctuating water. However, globally, we can also infer the general moving direction of the flow. This is highly related to the functionality of second-order motion. 

The isolated second-order motion may be rarely observed in natural environments; more often, it coexists with first-order motion as the optical fluctuation. 
We suggest that joint consideration of first- and second-order motion may potentially make the vision system more stable and reliable even in complex and noisy environments.

    

  </h2>
</section>



  

<hr class="gradient-divider">

<!-- 参考文献 BibTeX 格式 -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xxx,
  author    = {xxxx},
  title     = {HuPerFlow: a Human Perceived Flow dataset},
  journal   = {xxx},
  year      = {2025},
}</code></pre>
  </div>
</section>

<!-- 页脚部分 -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- 链接到PDF文件 -->
      <a class="icon-link" href="./static/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- GitHub图标及链接 -->
      <a class="icon-link" href="xxx" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- 网站许可证信息 -->
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
         
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
